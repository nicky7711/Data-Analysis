{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OpenStreetMap Data Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rhode Island, United States\n",
    "    - http://download.geofabrik.de/north-america/us/rhode-island-latest.osm.bz2\n",
    "I picked this data because last time I visited, the state was beautiful so I'd like to know more about Rhode Island."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I looked up the list of tags (I had no idea what tags mean), I could see that lots of data are very dirty (which means not consistent/no uniformity), so I decide to clean it up so that be able to get more accurate statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106: 1\n",
      "146: 1\n",
      "201: 1\n",
      "4: 1\n",
      "A: 1\n",
      "Alley: 2\n",
      "Ave: 68\n",
      "ave: 1\n",
      "Ave.: 4\n",
      "Avenue: 296\n",
      "Blvd: 2\n",
      "Boulevard: 18\n",
      "BowenStreet: 1\n",
      "Broadway: 4\n",
      "Circle: 10\n",
      "Court: 14\n",
      "Ct.: 1\n",
      "Dr: 12\n",
      "Dr.: 21\n",
      "Drive: 159\n",
      "Highway: 26\n",
      "HIGHWAY: 1\n",
      "Hill: 2\n",
      "Hwy: 1\n",
      "Island: 1\n",
      "Lane: 24\n",
      "Ln: 1\n",
      "Parkway: 5\n",
      "Pike: 22\n",
      "PIKE: 1\n",
      "Place: 4\n",
      "Plaza: 2\n",
      "Raod: 1\n",
      "Rd: 79\n",
      "Rd.: 1\n",
      "Road: 242\n",
      "road: 1\n",
      "Sq.: 1\n",
      "Square: 3\n",
      "St: 57\n",
      "St.: 9\n",
      "Street: 373\n",
      "Trail: 4\n",
      "Way: 50\n",
      "wht: 1\n",
      "Wy: 2\n"
     ]
    }
   ],
   "source": [
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "street_types = defaultdict(int)\n",
    "\n",
    "def audit_street_type(street_types, street_name):\n",
    "    \"\"\"\n",
    "    audit street type, and save all the types of data in street_types\n",
    "    \"\"\"\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        \n",
    "        street_types[street_type] += 1\n",
    "\n",
    "def print_sorted_dict(d):\n",
    "    \"\"\"\n",
    "    print sorted dictionary\n",
    "    \"\"\"\n",
    "    keys = d.keys()\n",
    "    keys = sorted(keys, key=lambda s: s.lower())\n",
    "    for k in keys:\n",
    "        v = d[k]\n",
    "        print (\"%s: %d\" % (k, v))\n",
    "\n",
    "def is_street_name(elem):\n",
    "    \"\"\"\n",
    "    it is a name of street only when the tag of the element is \"tag\" and its value of key is \"addr:street\"\n",
    "    \"\"\"\n",
    "    return (elem.tag == \"tag\") and (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "def audit():\n",
    "    \"\"\"\n",
    "    audit it\n",
    "    \"\"\"\n",
    "    for event, elem in ET.iterparse(osm_file):\n",
    "        if is_street_name(elem):\n",
    "            audit_street_type(street_types, elem.attrib['v'])    \n",
    "    print_sorted_dict(street_types)    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can see a lot! Now it's time to make auditing more specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "street_types = defaultdict(set)\n",
    "\n",
    "#the list of expected name of streeets\n",
    "expected_street_types = [\"Broadway\", \"Lane\", \"Hill\", \"Street\", \"Plaza\", \"Highway\", \"Way\", \"Trail\", \"Avenue\", \"Pike\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Road\", \"Lane\", \"Parkway\", \"Terrace\", \"Square\", \"Circle\"]\n",
    "#extended list of expected name of streets (description below)\n",
    "expected_street_types.extend(['A', '4', 'Alley'])\n",
    "def audit_street_type(street_types, street_name):\n",
    "    \"\"\"\n",
    "    same as above audit function, but at this time, save only those that is not in the list of expected name of streets\n",
    "    \"\"\"\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected_street_types:\n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "def audit():\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    return street_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "osm_file.seek(0)\n",
    "\n",
    "#List of dictionary for mapping (swapping the word)\n",
    "mapping = {\"Ave\": \"Avenue\",\n",
    "           \"Ave.\": \"Avenue\",\n",
    "           \"Blvd\": \"Boulevard\",\n",
    "           \"Dr\": \"Drive\",\n",
    "           \"Dr.\": \"Drive\",\n",
    "           \"Raod\": \"Road\",\n",
    "           \"Rd.\": \"Road\",\n",
    "           \"Sq.\": \"Square\",\n",
    "           \"Wy\": \"Way\",\n",
    "           \"St\": \"Street\",\n",
    "           \"St.\": \"Street\",\n",
    "           \"Rd\": \"Road\"\n",
    "            }\n",
    "\n",
    "mapping.update({\"BowenStreet\": \"Bowen Street\"})\n",
    "\n",
    "def update_name(name, mapping):\n",
    "    \"\"\"\n",
    "    swap the name by mapping\n",
    "    (ex. if the value is \"Rd\", swap it to the definition of \"Rd\" which is \"Road\")\n",
    "    \"\"\"\n",
    "    temp = \"\"\n",
    "    m = street_type_re.search(name)\n",
    "    if m:\n",
    "        for value in mapping:\n",
    "            if name.find(value) != -1:\n",
    "                temp = name[:name.index(value)] + mapping[value]\n",
    "    name = temp\n",
    "    return name\n",
    "\n",
    "def replace_word():\n",
    "    \"\"\"\n",
    "    replace the word and update it\n",
    "    \"\"\"\n",
    "    st_types = audit()\n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            update_name(name, mapping)\n",
    "#            better_name = update_name(name, mapping)\n",
    "#            print (name, \"=>\", better_name)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    replace_word()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Additional corrections: Avenue A, Lane 4, wht, Fones Alley, BowenStreet.\n",
    "#Avenue A, Lane 4, Fones Alley actually exist, so add on the list 'expected'\n",
    "#Add \"BowenStreet\": \"Bowen Street\" to dictionary\n",
    "#I couldn't find the definition of 'wht'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with the street! Next is the state name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similar to the street, but state at this time\n",
    "state = []\n",
    "osm_file.seek(0)\n",
    "\n",
    "def audit_state():\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        if elem.tag == \"way\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:state\":\n",
    "                    if tag.attrib['v'] != 'RI':\n",
    "                        state.extend([tag.attrib['v']])\n",
    "    return state\n",
    "\n",
    "mapping_state = {\"RO\": \"RI\",\n",
    "           \"ri\": \"RI\",\n",
    "           \"Rhode Island\": \"RI\",\n",
    "            }\n",
    "\n",
    "def update_state_name():\n",
    "    for i in range(0, len(state)):\n",
    "        state[i] = \"RI\"\n",
    "        \n",
    "def replace_state_name():\n",
    "    state = audit_state()\n",
    "    for ways in state.items():\n",
    "        for name in ways:\n",
    "            update_state_name(name, mapping)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    update_state_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done with state! Done with everything I've planned! Next one is to convert the cleaned data into csv, then start working on data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# OpenStreetMap Data Case Study\n",
    "First thing to see is the file size.\n",
    "## File Size\n",
    "```sql\n",
    "rhode-island-latest.osm .. 182 MB\n",
    "Project4 ................. 279 MB\n",
    "nodes.csv ................ 70 MB\n",
    "nodes_tags.csv ........... 2.4 MB\n",
    "ways.csv ................. 5.2 MB\n",
    "ways_tags.csv ............ 12 MB\n",
    "ways_nodes.cv ............ 22 MB  \n",
    "```\n",
    "\n",
    "## Did my Python code really work?\n",
    "To test if my python code really changes the data, I will try the following code. (my code cleanse the name of road, such that \"Blvd\"=>\"Boulevard\")\n",
    "```sql\n",
    "SELECT * \n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) tags\n",
    "WHERE value \n",
    "LIKE '%Blvd';\n",
    "```\n",
    "```sql\n",
    "19343479|name_type|Blvd|tiger\n",
    "19345366|name_type|Blvd|tiger\n",
    "19345373|name_type|Blvd|tiger\n",
    "19346665|name_type|Blvd|tiger\n",
    "19347232|name_type|Blvd|tiger\n",
    "19347234|name_type|Blvd|tiger\n",
    "19351170|name_type|Blvd|tiger\n",
    "19351284|name_type|Blvd|tiger\n",
    "19351289|name_type|Blvd|tiger\n",
    "19351916|name_type|Blvd|tiger\n",
    "19351928|name_type|Blvd|tiger\n",
    "19351936|name_type|Blvd|tiger\n",
    "19351942|name_type|Blvd|tiger\n",
    "```\n",
    "Only dump rows! I skim through the rows but couldn't find any value named \"Blvd\", which is dirty data before I cleaned it, so my previous python code fixed things correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data tags contain\n",
    "First thing I would like to test was the tags of each: nodes tags and ways tags, as I could not understand how they are organized. In order to see it, I decide to see the most frequently used keys and values, and possibly types, in nodes_tags table. In this case, in order to see how much nodes_tags table and ways_tags are different, I did not merge two tables for query.\n",
    "```sql\n",
    "SELECT count(id), key, value, type\n",
    "FROM nodes_tags\n",
    "GROUP BY key\n",
    "ORDER BY count(id) DESC;\n",
    "```\n",
    "```sql\n",
    "count|key|value|type\n",
    "6813|source|wind|generator\n",
    "6160|attribution|Office of Geographic and Environmental information (MassGIS)|regular\n",
    "4638|name|Shell|regular\n",
    "4466|power|generator|regular\n",
    "```\n",
    "I could see that tags in nodes are mostly used to describe what kind of building (facility) nodes are. Interestingly, most of them are used for power generator and MassGIS. On next, I will do the same procedure, but from ways_tags table, so that how much they are different.\n",
    "```sql\n",
    "count|key        |value|type\n",
    "41776|building   |yes|regular\n",
    "39782|highway    |service|regular\n",
    "30872|county     |Kent, RI|tiger\n",
    "30728|cfcc       |A41|tiger\n",
    "30069|name       |Our Lady of Mercy Catholic Parish|regular\n",
    "30002|reviewed   |no|tiger\n",
    "25914|name_base  |Frenchtown|tiger\n",
    "24494|name_type  |Rd|tiger\n",
    "18506|zip_left   |02818|tiger\n",
    "17282|zip_right  |02818|tiger\n",
    "5991 |source     |massgis_import_v0.1_20071009101303|regular\n",
    "4947 |upload_uuid|bulk_upload.pl-dd183b84-dae0-48c2-b387-c35f2e313537|tiger\n",
    "4939 |tlid       |58923536|tiger\n",
    "4235 |oneway     |yes|regular\n",
    "```\n",
    "Now I can see the difference more easily! However, most of data are hard to understand, suggesting need more data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who contributed the most\n",
    "Next thing is, to see who contributed to the OSM the most! I got this idea from the SQL example. However, I wanted to approach from both nodes and ways.\n",
    "```sql\n",
    "SELECT count(user), user\n",
    "FROM nodes\n",
    "GROUP BY user\n",
    "ORDER BY count(user) DESC;\n",
    "```\n",
    "```sql\n",
    "COUNT|NAME OF USER\n",
    "337837|woodpeck_fixbot\n",
    "212313|greggerm\n",
    "26848|Zirnch\n",
    "17328|maxerickson\n",
    "16771|John Wrenn\n",
    "16333|ZeLonewolf\n",
    "9693|morganwahl\n",
    "9432|Roman Guy\n",
    "9419|GeoStudent\n",
    "8439|TIGERcnl\n",
    "7229|jerryam\n",
    "7036|jremillard-massgis\n",
    "6787|42429\n",
    "6094|Alex KG Ellis\n",
    "5966|OMMB\n",
    "5620|MassGIS Import\n",
    "```\n",
    "I can see that woodpeck_fixbot (looks like a bot) contributed the most in the state of Rhode Island. greggerm, the second most contributor, however, looks like a human. I was interested in, so I personally looked up the history of his edit, and I could find out that he is actually a human. It is amazing how much he has contributed for osm, compared to other users.\n",
    "```sql\n",
    "COUNT|NAME OF USER\n",
    "37093|greggerm\n",
    "18472|bot-mode\n",
    "3328|DaveHansenTiger\n",
    "2601|Zirnch\n",
    "1759|maxerickson\n",
    "1645|GeoStudent\n",
    "1162|John Wrenn\n",
    "1058|Roman Guy\n",
    "1032|Alex KG Ellis\n",
    "1004|jremillard-massgis\n",
    "```\n",
    "In nodes_tables, greggerm won the bot! He had contributed more than double of bot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postal Code and the City\n",
    "Then I will compare the postal code and cities by counting how many of specific city/zip code are used, and matching those numbers.\n",
    "```sql\n",
    "SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "UNION ALL\n",
    "SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key like '%postcode%'\n",
    "group by tags.value\n",
    "order by count desc;\n",
    "```\n",
    "```sql\n",
    "ZIP CODE|COUNT\n",
    "02818|245\n",
    "02912|113\n",
    "02806|109\n",
    "02920|51\n",
    "02910|48\n",
    "02906|47\n",
    "02907|44\n",
    "02919|31\n",
    "```\n",
    "```sql\n",
    "SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "UNION ALL\n",
    "SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key like '%city%'\n",
    "group by tags.value\n",
    "order by count desc;\n",
    "```\n",
    "```sql\n",
    "CITY|COUNT\n",
    "East Greenwich|236\n",
    "Providence|201\n",
    "Barrington|108\n",
    "Portsmouth|95\n",
    "Cranston|72\n",
    "Warwick|48\n",
    "Pawtucket|33\n",
    "Johnston|31\n",
    "North Kingstown|30\n",
    "Newport|28\n",
    "```\n",
    "According to the Google Maps, 02818 = East Greenwich, 02912 = Providence, 02806 = Barrington, 02920 = Cranston. Assuming one city may have more than one postal code (i.e. Warwick has 3 zip codes: 02818, 02886, 02887, 02888, 02889) I could know that they are mostly correct. To prove it, I will run the following code:\n",
    "```sql\n",
    "SELECT tags.value, COUNT(*) as count\n",
    "FROM (SELECT * FROM nodes_tags\n",
    "UNION ALL\n",
    "SELECT * FROM ways_tags) tags\n",
    "WHERE tags.key like '%postcode%' AND (tags.value LIKE '%02886%' OR tags.value LIKE '%02888%' OR tags.value LIKE '%02889%' OR tags.value LIKE '%02887%' OR tags.value LIKE '%02818%')\n",
    "GROUP BY tags.value\n",
    "ORDER BY count DESC;\n",
    "```\n",
    "```sql\n",
    "ZIP CODE|COUNT\n",
    "02818|245\n",
    "02889|20\n",
    "02886|10\n",
    "02888|4\n",
    "```\n",
    "Assuming that Zip Code 02818 is a county mixed with the city of Warwick and East Greenwich, I can calculate the sum of counts of two citys (236+48=284) and the sume of counts of zip codes (245+20+10+4=279) I can see that they are mostly correct, with only 1.8% difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timestamp\n",
    "Next, I will see if there is any relationship between the timestamp and the number of contributes.\n",
    "```sql\n",
    "SELECT count(timestamp), timestamp \n",
    "FROM nodes \n",
    "GROUP BY timestamp \n",
    "HAVING count(timestamp) > 90 \n",
    "ORDER BY count(timestamp) desc;\n",
    "```\n",
    "```sql\n",
    "COUNT|TIME\n",
    "93|2016-07-21T03:44:54Z\n",
    "92|2016-07-22T20:04:24Z\n",
    "92|2016-09-02T18:35:24Z\n",
    "91|2016-07-13T19:48:23Z\n",
    "91|2016-12-23T13:31:17Z\n",
    "91|2016-12-24T17:28:43Z\n",
    "91|2017-01-25T15:25:23Z\n",
    "```\n",
    "93 contributions in one second, or 92, 91. It doesn't make any sense that a human made it (unless it was really popular website like Facebook or Instagram), I can assume that a bot made those modification at that specific time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of amenity\n",
    "Next thing is to get the number of amenities that have the most in the state.\n",
    "```sql\n",
    "SELECT value, COUNT(*) as num\n",
    "FROM nodes_tags\n",
    "WHERE key like '%amenity%'\n",
    "GROUP BY value\n",
    "ORDER BY num DESC\n",
    "LIMIT 20;\n",
    "```\n",
    "```sql\n",
    "AMENITY|COUNT\n",
    "school|602\n",
    "place_of_worship|506\n",
    "grave_yard|407\n",
    "restaurant|177\n",
    "library|115\n",
    "fire_station|112\n",
    "parking|88\n",
    "fast_food|78\n",
    "bench|72\n",
    "kindergarten|63\n",
    "cafe|58\n",
    "waste_basket|55\n",
    "fuel|53\n",
    "post_office|45\n",
    "police|36\n",
    "townhall|32\n",
    "bank|22\n",
    "pharmacy|17\n",
    "bicycle_parking|16\n",
    "social_facility|16\n",
    "```\n",
    "Lots of schools, place of worship, and grave yards are not really special, but the number of restaurants and fast food looks relevantly less than other states. This is only a guess, I need to look up same data from other state (or city as some cities are larger than Rhode Island) to prove it. But when I try only looking up the list of restaurants,\n",
    "```sql\n",
    "SELECT COUNT(*)\n",
    "FROM (SELECT * FROM nodes_tags UNION ALL SELECT * FROM ways_tags) tags\n",
    "WHERE value like '%restaurant%';\n",
    "```\n",
    "347.\n",
    "\n",
    "It suggests me there needs more data cleaning so that I could get more reliable statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggestion\n",
    "I could be able to improve by auditing more values, such as city names, zip code, type of amenity, and many others. By doing so, I will be able to get much more accurate and reliable statistics. I would suggest that, if OSM has such algorithm (not necessarily python, any language is fine) so that they filtered the data before a user or a bot inserts any, the data in OSM would be much better. However, I can see some anticipated issues as well. Every data in OSM is made by human, and human always makes mistake. If they make a mistake on the name of street (not the type of street, but the name of street itself!) no one will be able to catch it unless living in that area, so does my python or any other's code for data cleaning. One other thing that concerns is, there are so many exceptions on the actual street name. Avenue A, for example, is a legit name, but it gave me an error. There will be more than more exceptions, and it will require a lot more effort to clean them up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Although I did some of data cleaning, the data overview and statistics above show that there still needs more data and data cleaning, especially in the key, value pair of tags where the contributor has no limit to write anything. However, I still can say that my data cleaning has been helpful for getting more accurate statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
